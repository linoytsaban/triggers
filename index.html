<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>triggers</title>
    <style>
        html {
            scroll-behavior: smooth;
            height: 100%;
        }
        
        body {
            background-color: #000;
            color: #fff;
            font-family: 'Helvetica Neue', Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-attachment: fixed;
            background-position: center;
            background-repeat: no-repeat;
            background-size: cover;
            /* Background image will be set via GitHub when uploaded */
            /* The placeholder will be replaced with actual image */
            background-image: url('grid_portraits.png');
        }
        
        .container {
            max-width: 800px;
            padding: 40px;
            margin: 100vh auto 40px;
            background-color: rgba(0, 0, 0, 0.85);
            opacity: 0;
            animation: fadeIn 2s ease-in-out forwards;
            border-radius: 5px;
            box-shadow: 0 0 30px rgba(0, 0, 0, 0.8);
        }
        
        h1 {
            font-size: 2.5rem;
            font-weight: 300;
            letter-spacing: 2px;
            margin-bottom: 30px;
        }
        
        p {
            font-size: 1.1rem;
            margin-bottom: 20px;
            font-weight: 300;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        /* First section is full viewport height to showcase the background image */
        .hero {
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            position: relative;
        }
        
        .scroll-indicator {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            color: white;
            font-size: 14px;
            letter-spacing: 2px;
            opacity: 0.7;
            animation: bounce 2s infinite;
        }
        
        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% {transform: translateY(0) translateX(-50%);}
            40% {transform: translateY(-20px) translateX(-50%);}
            60% {transform: translateY(-10px) translateX(-50%);}
        }
        
        /* Add subtle hover effect to text to represent "triggering" */
        p:hover {
            color: #f0f0f0;
            text-shadow: 0 0 8px rgba(255,255,255,0.5);
            transition: all 0.3s ease;
        }
    </style>
</head>
<body>
    <div class="hero">
        <div class="scroll-indicator">SCROLL DOWN</div>
    </div>
    <div class="container">
        <h1>triggers</h1>
        
        <p>"triggers" explores the profound parallel between trauma's invisible rewiring of human perception and selective modifications of artificial neural networks. This work examines how traumatic experiences fundamentally alter our processing of sensory input—where ordinary stimuli like locations, smells, or sounds become powerful triggers that activate overwhelming emotional and physiological responses.</p>
        
        <p>Just as trauma creates these heightened associative pathways in the human mind—where certain sensory channels become hypersensitive while others may become muted—this artwork intervenes in the attention mechanisms of CLIP image encoder to simulate this perceptual rewiring. By selectively amplifying and silencing specific attention heads within the neural network, the visual output manifests the invisible yet profound alteration of perception that trauma survivors experience.</p>
        
        <p>Historically, self-portraiture has served as a powerful vehicle for expressing psychological states and emotional distress. Artists have long turned to the human face as a canvas for exploring interior states. This work draws inspiration from this tradition, using computational manipulation to fragment and distort facial representation in ways that echo how trauma disrupts one's self-perception and relationship to others. The resulting imagery invites viewers to contemplate how trauma fundamentally changes our relationship with the world around us, creating invisible but powerful filters through which all subsequent experience must pass. Through this analogy, "triggers" offers a visualization of trauma's enduring impact—revealing how certain channels of perception become permanently heightened while others fade into silence, fundamentally altering our experience of reality without leaving visible traces.</p>

        <p>The grid structure, with stark variations between adjacent portraits, seeks to create a state of visual overstimulation. The viewer's gaze is pulled in multiple competing directions simultaneously, unable to settle on a single coherent representation—mirroring the overwhelming cognitive load experienced during trauma responses, where normal integrative processing breaks down under the weight of competing sensory information.</p>
        
        <p style="font-style: italic;">The inspiration for this work stems from my personal experiences with trauma. Post-traumatic responses often manifest as overwhelming sensations that either silence everything else or amplify them to unbearable levels. These responses can be activated by various triggers associated with the original trauma. I perceive the semantic roles within CLIP as analogous to these triggers—each capable of dramatically altering perception when activated or suppressed, just as trauma reconfigures our sensory processing in profound and often invisible ways.</p>

        <h2>technical overview</h2>

        <p>CLIP (Contrastive Language-Image Pre-training) encoders are neural networks trained on diverse image-text pairs, creating a powerful bridge between visual content and textual descriptions. The architecture combines a vision encoder and a text encoder, jointly trained to align embeddings from both modalities in a shared representation space. Within CLIP's transformer architecture, attention mechanisms play a crucial role in processing and interpreting visual information.</p>

        <p>The exploration of semantically meaningful directions in latent spaces began with GANs (Goodfellow et al., 2014; Karras et al., 2019), where researchers discovered that moving along specific trajectories enabled controlled image editing operations (Shen et al., 2020). CLIP (Radford et al., 2021) revolutionized this approach by unifying visual and textual representations, enabling text-guided manipulation and spawning numerous applications—from finding traversal directions that align images with text descriptions to enabling zero-shot domain adaptation.</p>
        
        <p>Building on this foundation, Gandelsman et al. recently conducted a granular investigation of CLIP's image encoder, analyzing how individual components contribute to the final representation. Their work characterized each attention head's specific role by algorithmically identifying text representations spanning its output space, revealing that many heads specialize in particular properties such as location or shape.</p>
    
        <p>To explore the potential visual effects associated with these semantic roles, the insights were paired with the IP adapter mechanism—a component facilitating image generation conditioning through reference images that allows visual characteristics to guide new image creation. The proposed algorithm's outputs for the CLIP ViT-H encoder were analyzed, with common themes and patterns within each attention head's outputs being identified through LLM-based analysis. A scaling operation was subsequently implemented for each attention head output, enabling the muting or amplification of specific heads (such as those focused on 'colors') when provided with the relevant attention layer and head indices, thereby modifying the resulting CLIP image embeddings accordingly.</p>

        <p>These manipulated embeddings were then incorporated as input to a generation pipeline, wherein an IP adapter and ControlNet were utilized to create portraits. Conditioning for each portrait was established through both a reference face image (thereby maintaining structural similarity across generations) and the manipulated embeddings carrying the semantic modifications.</p>

        <p>The grid of portraits displayed in this work demonstrates this process, with each image being generated through the amplification or silencing of different groups of attention heads within CLIP's vision transformer. Text prompts and face conditioning were maintained constant across all generations, with the manipulated CLIP embeddings serving as the sole variable—thus effectively visualizing how selective attention modifications parallel the perceptual alterations experienced in traumatic processing.</p>

        <div style="text-align: center; margin: 40px 0;">
            <img src="example.png" alt="attention modification example" style="width: 60%; height: auto;" />
        </div>
    </div>
</body>
</html>
